{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/mushroom_csv.csv')"
      ],
      "metadata": {
        "id": "VSG0jDEMmtW3"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "# Initialize LabelEncoder\n",
        "label_encoders = {}\n",
        "for column in df.columns:\n",
        "    label_encoders[column] = LabelEncoder()\n",
        "    df[column] = label_encoders[column].fit_transform(df[column])\n",
        "\n",
        "# Separate features and target variable\n",
        "X = df.drop('class', axis=1)  # features\n",
        "y = df['class']  # target variable\n",
        "\n",
        "\n",
        "# Sigmoid function\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Cost function (Mean Squared Error)\n",
        "def mse(y, t):\n",
        "    return 0.5 * np.mean((y - t) ** 2)\n",
        "\n",
        "# Derivative of the sigmoid function for backpropagation\n",
        "def sigmoid_derivative(z):\n",
        "    return sigmoid(z) * (1 - sigmoid(z))\n",
        "\n",
        "# Gradient descent update rule for logistic regression\n",
        "def update_weights(X, y, t, w, b, learning_rate):\n",
        "    # Calculate predictions\n",
        "    predictions = sigmoid(np.dot(X, w) + b)\n",
        "\n",
        "    # Calculate error\n",
        "    error = y - t\n",
        "\n",
        "    # Calculate gradients\n",
        "    gradient_w = np.dot(X.T, error * sigmoid_derivative(predictions))\n",
        "    gradient_b = np.sum(error * sigmoid_derivative(predictions))\n",
        "\n",
        "    # Update weights and bias\n",
        "    w -= learning_rate * gradient_w\n",
        "    b -= learning_rate * gradient_b\n",
        "\n",
        "    return w, b\n"
      ],
      "metadata": {
        "id": "g6-ITjNwYh5r"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main function to train logistic regression model\n",
        "def train_logistic_regression(X, t, learning_rate=0.01, epochs=1000):\n",
        "    # Initialize weights and bias\n",
        "    w = np.zeros(X.shape[1])\n",
        "    b = 0\n",
        "\n",
        "    # Perform gradient descent\n",
        "    for epoch in range(epochs):\n",
        "        # Randomly shuffle data\n",
        "        indices = np.arange(X.shape[0])\n",
        "        np.random.shuffle(indices)\n",
        "        X = X[indices]\n",
        "        t = t[indices]\n",
        "\n",
        "        # Update weights and bias for each instance\n",
        "        for i in range(X.shape[0]):\n",
        "            w, b = update_weights(X[i:i+1], sigmoid(np.dot(X[i:i+1], w) + b), t[i:i+1], w, b, learning_rate)\n",
        "\n",
        "        # Print cost every 100 epochs\n",
        "        if epoch % 100 == 0:\n",
        "            cost = mse(sigmoid(np.dot(X, w) + b), t)\n",
        "            print(f'Epoch {epoch}, Cost: {cost}')\n",
        "\n",
        "    return w, b"
      ],
      "metadata": {
        "id": "5PIqHdMHZD5H"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Split the dataset into training set and test set with a test size of 30%\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Display the transformed features and target variable\n",
        "X_train.head(), y_train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_yJjvrjZ709",
        "outputId": "6fa8b4be-439d-4757-8049-7fcd47f82327"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(      cap-shape  cap-surface  cap-color  bruises%3F  odor  gill-attachment  \\\n",
              " 5921          5            2          0           1     2                1   \n",
              " 1073          5            0          3           1     5                1   \n",
              " 3710          5            0          3           0     2                1   \n",
              " 144           5            3          9           1     0                1   \n",
              " 5469          5            3          4           0     8                1   \n",
              " \n",
              "       gill-spacing  gill-size  gill-color  stalk-shape  ...  \\\n",
              " 5921             0          0           3            1  ...   \n",
              " 1073             0          0           7            1  ...   \n",
              " 3710             0          0           7            0  ...   \n",
              " 144              0          0           4            0  ...   \n",
              " 5469             0          1           0            1  ...   \n",
              " \n",
              "       stalk-surface-below-ring  stalk-color-above-ring  \\\n",
              " 5921                         0                       7   \n",
              " 1073                         2                       6   \n",
              " 3710                         1                       4   \n",
              " 144                          2                       7   \n",
              " 5469                         1                       7   \n",
              " \n",
              "       stalk-color-below-ring  veil-type  veil-color  ring-number  ring-type  \\\n",
              " 5921                       7          0           2            1          4   \n",
              " 1073                       7          0           2            1          4   \n",
              " 3710                       0          0           2            1          2   \n",
              " 144                        7          0           2            1          4   \n",
              " 5469                       6          0           2            1          0   \n",
              " \n",
              "       spore-print-color  population  habitat  \n",
              " 5921                  1           4        5  \n",
              " 1073                  2           5        0  \n",
              " 3710                  1           4        0  \n",
              " 144                   2           2        1  \n",
              " 5469                  7           4        0  \n",
              " \n",
              " [5 rows x 22 columns],\n",
              " 5921    1\n",
              " 1073    0\n",
              " 3710    1\n",
              " 144     0\n",
              " 5469    1\n",
              " Name: class, dtype: int64)"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Initialize the logistic regression model with stochastic gradient descent\n",
        "log_reg_sgd = SGDClassifier(loss='log', max_iter=1000, tol=1e-3, random_state=42)\n",
        "\n",
        "# Fit the model to the training data\n",
        "log_reg_sgd.fit(X_train, y_train)\n",
        "\n",
        "# Predict the class labels for the test set\n",
        "y_pred = log_reg_sgd.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model on the test set\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Output the accuracy\n",
        "accuracy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o38GcgTAo8gx",
        "outputId": "6013ec6a-79c3-4f79-e602-e8c084373e20"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9589827727645611"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    }
  ]
}